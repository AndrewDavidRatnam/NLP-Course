{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJADkRLgiiheZkqlFjF52a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndrewDavidRatnam/NLP-Course/blob/main/NLP_Summary_Series.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP LECTURE SUMMARY SERIES\n",
        "\n",
        "[Natural Language Processing, IIT Kharagpur](https://nptel.ac.in/courses/106105158)\n",
        "\n",
        "# üß† Natural Language Processing (NLP): A Comprehensive Overview\n",
        "\n",
        "---\n",
        "\n",
        "## 1. What is NLP?\n",
        "\n",
        "**Analogy:**  \n",
        "Think of NLP as a **multilingual chef in a bustling kitchen**. You place an order using slang or vague terms (\"something spicy\"), and the chef understands, translates, and serves a personalized dish‚Äîwhether it‚Äôs a **response**, **summary**, or **action**.\n",
        "\n",
        "**Core Idea:**  \n",
        "NLP is a field of AI that enables computers to **process**, **understand**, and **generate** human language using linguistics, computer science, and machine learning.\n",
        "\n",
        "**Real-World Examples:**\n",
        "- **Chatbots:** \"Where‚Äôs my package?\" ‚Üí Understands & responds.\n",
        "- **Voice Assistants:** \"Play some jazz\" ‚Üí Interprets & plays music.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Key Components of NLP\n",
        "\n",
        "Each task in NLP is like a **tool in a chef‚Äôs kitchen**:\n",
        "\n",
        "### a. Tokenization  \n",
        "**Analogy:** Like chopping vegetables into pieces.  \n",
        "**Function:** Breaks text into words, phrases, or symbols.\n",
        "\n",
        "**Example:**  \n",
        "Input: `\"best Italian restaurants near me\"`  \n",
        "Tokens: `[\"best\", \"Italian\", \"restaurants\", \"near\", \"me\"]`\n",
        "\n",
        "---\n",
        "\n",
        "### b. Part-of-Speech (POS) Tagging  \n",
        "**Analogy:** Labeling ingredients as \"protein,\" \"spice,\" etc.  \n",
        "**Function:** Identifies parts of speech: nouns, verbs, etc.\n",
        "\n",
        "**Example:**  \n",
        "\"She run fast\" ‚Üí Tags \"run\" as verb ‚Üí Suggests correction to \"runs\"\n",
        "\n",
        "---\n",
        "\n",
        "### c. Named Entity Recognition (NER)  \n",
        "**Analogy:** Tour guide highlighting landmarks.  \n",
        "**Function:** Detects names of people, places, and organizations.\n",
        "\n",
        "**Example:**  \n",
        "‚ÄúElon Musk is CEO of Tesla‚Äù ‚Üí  \n",
        "- Person: Elon Musk  \n",
        "- Organization: Tesla\n",
        "\n",
        "---\n",
        "\n",
        "### d. Sentiment Analysis  \n",
        "**Analogy:** A critic gauging audience mood.  \n",
        "**Function:** Determines if a text is positive, negative, or neutral.\n",
        "\n",
        "**Example:**  \n",
        "Tweet: \"Love the new update!\" ‚Üí **Positive Sentiment**\n",
        "\n",
        "---\n",
        "\n",
        "### e. Machine Translation  \n",
        "**Analogy:** Real-time interpreter at a global conference.  \n",
        "**Function:** Translates text between languages, preserving meaning.\n",
        "\n",
        "**Example:**  \n",
        "\"It‚Äôs raining cats and dogs\" ‚Üí  \n",
        "Translated to Spanish idiomatically: `\"Est√° lloviendo a c√°ntaros\"`\n",
        "\n",
        "---\n",
        "\n",
        "## 3. How Does NLP Work?\n",
        "\n",
        "**Analogy:** NLP is like a child learning to talk‚Äîstarting with mimicry, then patterns, then context.\n",
        "\n",
        "### Core Process:\n",
        "1. **Input:** e.g., `\"Book a flight to Paris\"`\n",
        "2. **Preprocessing:** Tokenization, removing noise\n",
        "3. **Analysis:** Extracting meaning (e.g., \"Paris\" = location)\n",
        "4. **Output:** Response (e.g., show flight options)\n",
        "\n",
        "**Example:**  \n",
        "Autocomplete in email: `\"Looking forward to...\"` ‚Üí Predicts `\"our meeting\"`\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Why is NLP Challenging?\n",
        "\n",
        "**Analogy:** Human language is a flowing river with hidden currents (ambiguity, context, sarcasm).\n",
        "\n",
        "### Key Challenges:\n",
        "- **Ambiguity:** \"Bank\" = riverbank or financial institution?\n",
        "- **Cultural Nuances:** Sarcasm, idioms differ by region\n",
        "- **Bias in Data:** Leads to biased outputs\n",
        "\n",
        "**Example:**  \n",
        "Naive bot misreads sarcasm:  \n",
        "\"Great job, my package is lost again\" ‚Üí Interpreted as positive.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. NLP in Action Today\n",
        "\n",
        "**Analogy:** NLP is like a **Swiss Army knife**‚Äîversatile & everywhere.\n",
        "\n",
        "### Use Cases:\n",
        "- **Healthcare:** Flag symptoms in doctor‚Äôs notes.\n",
        "- **Education:** Language apps like Duolingo.\n",
        "- **Entertainment:** Netflix analyzing reviews for recommendations.\n",
        "- **Security:** Detecting phishing via suspicious text patterns.\n",
        "\n",
        "**Example:**  \n",
        "E-commerce: NLP recommends scarves when you buy a coat.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. The Future of NLP\n",
        "\n",
        "**Analogy:** If NLP today is a translator, tomorrow it will be a **mind-reader**‚Äîdetecting tone, emotion, and intent.\n",
        "\n",
        "### Trends:\n",
        "- **Multimodal NLP:** Text + image + voice\n",
        "- **Low-Resource Languages:** Support for underrepresented languages\n",
        "- **Ethical NLP:** Fairness, bias reduction\n",
        "\n",
        "**Example:**  \n",
        "A future universal translator that interprets **tone**, **humor**, and **context** in real-time.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ú® Final Thoughts\n",
        "\n",
        "NLP is the art and science of making machines fluent in human language‚Äîlike teaching a robot to join dinner conversation.\n",
        "\n",
        "From **search engines** to **summarization apps**, NLP **breaks down** language (like prepping ingredients) and **rebuilds** meaning (like serving a dish)‚Äîhelping machines **understand us** as we understand each other.\n",
        "\n",
        "---\n",
        "\n",
        "## Want to Dive Deeper? üîç STAY TUNED!!\n",
        "\n",
        "- How neural networks power modern chatbots  \n",
        "- Build a simple NLP model in Python  \n",
        "- Explore specific tasks like summarization, translation, or sentiment analysis\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fHzDZMEnEe38"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LECTURE 2 CODE AND SCRIPT IN YOUTUBE VIDEO AND ALSO ALL RELVANT INFORMATION INFORMATION"
      ],
      "metadata": {
        "id": "PENaWgBpFlsR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üé¨ [YouTube Video Script: ‚ÄúWhy NLP is Hard: Translation Fails, Chatbot Chaos, and Real Wins‚Äù]\n",
        "[INTRO: 0:00 ‚Äì 0:25]\n",
        "\n",
        "üéµ [Light, quirky music]\n",
        "üé• [Visuals: Funny translations, a confused chatbot, a tweet storm]\n",
        "\n",
        "Narrator (playful tone):\n",
        "\"Imagine saying ‚ÄúGoogle is awesome‚Äù and your translator responds with‚Ä¶ ‚ÄòGoogle Bionic‚Äô? üò≥ Yeah, that actually happened.\"\n",
        "\n",
        "üìå In this episode, we uncover why NLP ‚Äî Natural Language Processing ‚Äî is so hard, yet so powerful. From chatbot fails to translation blunders, and surprisingly successful tools you use every day.\n",
        "\n",
        "[SECTION 1: WHY NLP IS HARD | 0:25 ‚Äì 1:30]\n",
        "\n",
        "üé• [Visuals: Split-screen of English vs. bad translations]\n",
        "\n",
        "Narrator:\n",
        "\"NLP isn‚Äôt just about turning text into data ‚Äî it‚Äôs about understanding context. And context is... messy.\"\n",
        "\n",
        "üí¨ Take Google Translate:\n",
        "Say ‚ÄúGoogle is cool‚Äù ‚Äî but in Hindi, you get ‚ÄúGoogle champagne.‚Äù üçæ Why?\n",
        "Because ‚Äòcool‚Äô could mean trendy‚Ä¶ or cold‚Ä¶ or a drink apparently.\n",
        "\n",
        "üß† This is called Word Sense Disambiguation ‚Äî and it‚Äôs one of NLP‚Äôs biggest challenges.\n",
        "\n",
        "[SECTION 2: EVEN HUMANS STRUGGLE | 1:30 ‚Äì 2:00]\n",
        "\n",
        "üé• [Visuals: Ad blunders from global brands]\n",
        "\n",
        "Narrator:\n",
        "\"And guess what? Humans mess this up too.\"\n",
        "\n",
        "Pepsi‚Äôs famous blunder in China?\n",
        "Their slogan ‚ÄúCome Alive with the Pepsi Generation‚Äù got translated to‚Ä¶\n",
        "üëâ ‚ÄúPepsi brings your ancestors back from the dead.‚Äù\n",
        "\n",
        "üìå So if humans struggle with context, how do we expect machines to do better?\n",
        "\n",
        "[SECTION 3: NLP GOALS ‚Äì AMBITION VS. REALITY | 2:00 ‚Äì 3:00]\n",
        "\n",
        "üé• [Visuals: Microsoft Tay headlines, a chatbot gone wrong]\n",
        "\n",
        "Narrator:\n",
        "\"There‚Äôs a huge gap between what we want NLP to do and what it can realistically do.\"\n",
        "\n",
        "üåç Ambitious Goal? A perfect, open-domain chatbot like Tay from Microsoft.\n",
        "üò¨ Spoiler: It went rogue in less than 24 hours.\n",
        "\n",
        "‚úÖ Practical Goal? Tools that just work, like:\n",
        "\n",
        "Query correction: ‚Äúwold cup‚Äù ‚Üí ‚Äúworld cup‚Äù\n",
        "\n",
        "Query completion: Predicting ‚Äúhow to train your‚Ä¶‚Äù ‚Üí ‚Äúdragon‚Äù\n",
        "\n",
        "üìå Simple, useful, reliable ‚Äî that‚Äôs the current sweet spot for NLP.\n",
        "\n",
        "[SECTION 4: REAL-WORLD WINS | 3:00 ‚Äì 4:15]\n",
        "\n",
        "üé• [Visuals: News articles, code snippets, chatbot interfaces]\n",
        "\n",
        "Narrator:\n",
        "\"Let‚Äôs talk about what‚Äôs working.\"\n",
        "\n",
        "üì∞ Information Extraction\n",
        "From a sentence like:\n",
        "‚ÄúJane Doe, President of Acme Corp, resigned today‚Ä¶‚Äù\n",
        "NLP can pull out:\n",
        "\n",
        "Name: Jane Doe\n",
        "\n",
        "Role: President\n",
        "\n",
        "Company: Acme Corp\n",
        "\n",
        "ü§ñ Domain-specific Chatbots\n",
        "A course assistant bot that answers student queries with 97% accuracy? That‚Äôs real. That‚Äôs useful.\n",
        "Unlike Tay.\n",
        "\n",
        "üê¶ Sentiment Analysis\n",
        "By analyzing tweets, NLP can predict election outcomes, track public mood, or detect market trends.\n",
        "\n",
        "üì¨ Spam Detection\n",
        "Gmail uses NLP to filter spam ‚Äî so you don‚Äôt have to.\n",
        "\n",
        "üìÑ Text Summarization\n",
        "Apps like Inshorts condense long news into short summaries ‚Äî using NLP.\n",
        "\n",
        "[SECTION 5: NLP IN ACTION (LIVE DEMO!) | 4:15 ‚Äì 5:15]\n",
        "\n",
        "üé• [Screen recording of Python code running]\n",
        "\n",
        "Narrator:\n",
        "\"Want a peek under the hood? Here‚Äôs a basic NLP demo using Python.\"\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "text = \"Google is awesome but translation needs work!\"\n",
        "tokens = word_tokenize(text)\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "scores = sid.polarity_scores(text)\n",
        "print(\"Sentiment Scores:\", scores)\n",
        "üß™ We tokenize the text and analyze its sentiment. The result? Positive vibes with a hint of criticism ‚Äî just like real feedback.\n",
        "\n",
        "[SECTION 6: PRACTICALITY OVER PERFECTION | 5:15 ‚Äì 6:00]\n",
        "\n",
        "üé• [Visuals: Tools that ‚Äúmostly‚Äù work ‚Äî Gmail, search, summaries]\n",
        "\n",
        "Narrator:\n",
        "\"Perfect NLP doesn‚Äôt exist. But ‚Äògood enough‚Äô NLP? That powers our digital world.\"\n",
        "\n",
        "üìå Translation may be flawed, but we still use it.\n",
        "üìå Chatbots aren‚Äôt perfect, but they save time.\n",
        "üìå NLP isn‚Äôt magic ‚Äî it‚Äôs clever algorithms, smart design, and lots of data.\n",
        "\n",
        "üë®‚Äçüè´ And that‚Äôs what this course teaches ‚Äî not just how to use NLP, but how to build it, improve it, and make it useful.\n",
        "\n",
        "[OUTRO: 6:00 ‚Äì 6:30]\n",
        "\n",
        "üéµ [Inspiring outro music]\n",
        "üé• [Montage of successful NLP tools, ending on a student building one]\n",
        "\n",
        "Narrator (motivational):\n",
        "\"NLP is hard ‚Äî but not impossible. Start with what works. Build toward what‚Äôs better.\"\n",
        "\n",
        "üëç Like this video if you‚Äôve ever laughed at a bad translation. Subscribe for more deep dives into the future of language and AI.\n",
        "\n",
        "üìò Next up: Why algorithms matter ‚Äî and how they power NLP."
      ],
      "metadata": {
        "id": "lXalAWBjF3DQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Lecture 2: What Do We Do in NLP?\n",
        "####Summary:\n",
        "\n",
        "Lecture 2 introduces the practical tasks NLP practitioners undertake, building on Lecture 1‚Äôs overview by illustrating the engineering goal of building systems for real-world use. The instructor uses translation errors, like Google Translate rendering ‚ÄúGoogle is awesome‚Äù as ‚ÄúGoogle Bionic‚Äù in Hindi, to highlight NLP‚Äôs imperfections and challenges, such as word sense disambiguation. Human translation blunders, like Pepsi‚Äôs slogan becoming ‚ÄúPepsi brings your relatives back from the dead‚Äù in Chinese, show that even humans struggle with context, underscoring NLP‚Äôs complexity. The lecture contrasts ambitious goals, like perfect translation or open-domain chatbots (e.g., Microsoft‚Äôs failed Tay), with achievable ones, like query correction in search engines. Practical applications are emphasized, including information extraction to structure data from news reports, such as identifying a person‚Äôs role in a company. Domain-specific chatbots, like a course TA bot answering student queries with 97% accuracy, are presented as feasible NLP successes. Sentiment analysis is introduced as a tool to gauge opinions from social media, exemplified by studies predicting election outcomes. Other tasks, like spam detection and text summarization, are noted as widely used in daily life, from email filters to news apps. The instructor stresses that while NLP systems aren‚Äôt perfect, they‚Äôre ‚Äúgood enough‚Äù for many applications, encouraging students to design innovative solutions. The lecture sets the stage for future discussions on why NLP is hard, promising deeper dives into algorithms and challenges.\n",
        "\n",
        "####Key Concepts:\n",
        "\n",
        "Engineering Goal: Designing systems to process natural language for practical applications.\n",
        "Translation Challenges: Imperfect translations (e.g., ‚ÄúGoogle is awesome‚Äù ‚Üí ‚ÄúGoogle Bionic‚Äù) due to context errors.\n",
        "Word Sense Disambiguation: Resolving multiple meanings of words (e.g., ‚Äúcool‚Äù as trendy vs. cold).\n",
        "Human Errors: Translation blunders (e.g., Pepsi‚Äôs slogan) show context is hard for humans too.\n",
        "Ambitious Goals: Perfect translation or open-domain chatbots (e.g., Tay‚Äôs failure) remain elusive.\n",
        "Query Correction: Fixing misspellings in search queries (e.g., ‚Äúwold‚Äù ‚Üí ‚Äúworld‚Äù).\n",
        "Query Completion: Predicting full queries from partial inputs using language modeling.\n",
        "Information Extraction: Structuring unstructured text, like extracting job titles from news (e.g., ‚Äúpresident, Times newspaper‚Äù).\n",
        "Domain-Specific Chatbots: Limited-scope bots (e.g., course TA bot) achieve high accuracy.\n",
        "Sentiment Analysis: Classifying opinions from social media to predict trends (e.g., election outcomes).\n",
        "Spam Detection: Filtering unwanted emails or comments via text analysis.\n",
        "Text Summarization: Condensing articles into key points for news or research.\n",
        "Practicality Over Perfection: NLP systems are imperfect but useful for many tasks.\n",
        "Algorithm Design: Solving NLP problems requires efficient, tailored algorithms.\n",
        "Real-World Examples:\n",
        "\n",
        "Google Translate: Struggles with phrases like ‚ÄúGoogle is cool‚Äù (translated as ‚ÄúGoogle champagne‚Äù), showing the need for better disambiguation.\n",
        "Search Engines (Google): Corrects ‚Äúwold cup‚Äù to ‚Äúworld cup‚Äù and predicts query completions for faster searches.\n",
        "News Analysis: Extracts structured data (e.g., ‚ÄúJohn is CEO of Acme‚Äù) from articles for databases.\n",
        "Course Chatbot: A TA bot answers student queries with 97% accuracy, showing domain-specific success.\n",
        "Election Sentiment (Twitter): Analyzes tweets to predict voter preferences, as seen in presidential campaigns.\n",
        "Gmail Spam Filter: Detects spam emails by analyzing text patterns, keeping inboxes clean.\n",
        "\n",
        "###Explaining the Concept: What Do We Do in NLP?\n",
        "\n",
        "####Analogy:\n",
        "\n",
        "Imagine NLP practitioners as detectives sifting through a noisy crime scene‚Äîtext data scattered across emails, tweets, and articles. Their job is to collect clues (tokenize words), identify key players (extract entities), and piece together the story (summarize or classify sentiment), all while knowing the evidence isn‚Äôt perfect. Like detectives using magnifying glasses (algorithms), they clean up smudges (preprocessing) to spot patterns, such as a suspect‚Äôs alias having multiple meanings (disambiguation). They might correct a witness‚Äôs typo (query correction) or predict what someone‚Äôs hiding based on partial clues (query completion). Sometimes, they build a case file (structured data) from chaotic notes, like linking a name to a job title. Other times, they gauge the crowd‚Äôs mood (sentiment analysis) to sense tension. Their tools, like fingerprint kits (NLTK, spaCy), help crack cases, but mistakes‚Äîlike misreading ‚Äúcool‚Äù as ‚Äúchampagne‚Äù‚Äîremind them context is tricky. Unlike fictional sleuths solving perfect mysteries, NLP detectives deliver ‚Äúgood enough‚Äù reports (applications) that help real people, from filtering spam to translating news. They don‚Äôt aim for flawless truth but for practical insights, like a chatbot calming a customer or a summary saving time. Each task, from cleaning to analyzing, is a step toward making sense of the world‚Äôs messy language.\n",
        "\n",
        "####Detailed Explanation with Analogies:\n",
        "\n",
        "Text as a Crime Scene: Raw text is chaotic, like evidence strewn about. NLP practitioners tokenize it (collect clues into words) and clean it (remove noise like punctuation), setting the stage for analysis, as seen in the lecture‚Äôs translation examples needing preprocessing.\n",
        "Disambiguation as a Suspect Lineup: Words like ‚Äúcool‚Äù have multiple identities (trendy, cold). Practitioners pick the right one using context, just as the lecture notes Google Translate‚Äôs errors stem from picking wrong senses.\n",
        "Extraction as Filing Reports: Information extraction organizes messy text into neat records, like a detective noting ‚ÄúJohn, CEO, Acme‚Äù from a rambling article, mirroring the lecture‚Äôs example of structuring news data.\n",
        "Sentiment as Reading the Room: Practitioners classify text‚Äôs vibe‚Äîpositive, negative‚Äîlike detectives sensing a crowd‚Äôs mood. The lecture‚Äôs election analysis shows this, predicting outcomes from tweets.\n",
        "Chatbots as Assistants: Domain-specific chatbots, like the course TA bot, are junior detectives trained for one case (a course), answering reliably, unlike open-domain bots (Tay) that go rogue, as the lecture contrasts.\n",
        "####Real-World Examples:\n",
        "\n",
        "Customer Service (Amazon): Chatbots use tokenization and entity recognition to parse ‚ÄúWhere‚Äôs my package?‚Äù and fetch tracking details, streamlining support.\n",
        "YouTube Moderation: Spam detection flags toxic comments by analyzing patterns, keeping platforms safe, as hinted in the lecture‚Äôs spam discussion.\n",
        "News Apps (BBC): Text summarization condenses articles into headlines, using extraction to highlight key events, aligning with the lecture‚Äôs practical goals.\n",
        "Bing Search: Query completion suggests ‚ÄúWorld Cup 2022‚Äù when typing ‚ÄúWorld C,‚Äù leveraging language modeling, as the lecture describes.\n",
        "Marketing Analytics: Sentiment analysis scans reviews to gauge product reception, guiding campaigns, like the lecture‚Äôs social media example.\n",
        "####Connection to Prior Lessons:\n",
        "Lecture 1 outlined NLP‚Äôs dual goals‚Äîscientific (understanding language) and engineering (building tools)‚Äîand promised applications like summarization and opinion mining. Lecture 2 delivers by showing what practitioners do to achieve the engineering goal, using examples like query correction and chatbots. The detective analogy extends Lecture 1‚Äôs view of text as abundant but messy, requiring practical steps (e.g., Lecture 1‚Äôs tokenization) to solve real problems. The lecture‚Äôs focus on imperfect but useful systems echoes Lecture 1‚Äôs engineering emphasis, grounding promised topics in tangible tasks.\n"
      ],
      "metadata": {
        "id": "ON-DRr__F565"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CODE EXAMPLE 1 SENTIMENT ANALYSIS"
      ],
      "metadata": {
        "id": "7QGdg_BrIS6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('punkt_tab')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a84UCqRnHR_s",
        "outputId": "9cf454e6-45bb-4591-e1ba-2ebe90fc3300"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Google is awesome but translation needs work\"\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "print(\"Tokens:\", tokens)\n",
        "print(\"\")\n",
        "\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "scores = sid.polarity_scores(text)\n",
        "print(\"Sentiment Scores:\", scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rd6SKfjWIPeK",
        "outputId": "fe78bf26-a646-4274-8fc0-1af0d7a2f7bd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['Google', 'is', 'awesome', 'but', 'translation', 'needs', 'work']\n",
            "\n",
            "Sentiment Scores: {'neg': 0.0, 'neu': 0.702, 'pos': 0.298, 'compound': 0.3716}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CODE EXAMPLE : QUERY COMPLETION"
      ],
      "metadata": {
        "id": "lL7yqHyqIYOH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from difflib import get_close_matches"
      ],
      "metadata": {
        "id": "6x1eno2zIZxG"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dictonary = [\"world\", \"cup\", \"football\", \"2024\"]\n",
        "\n",
        "mispelled_query = \"wold cup\"\n",
        "\n",
        "words = mispelled_query.split()\n",
        "words\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYzlu0_LIhIm",
        "outputId": "59a642f6-558d-4de4-dbbe-86194338753c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['wold', 'cup']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corrected_query = []\n",
        "for word in words:\n",
        "  corrected_query.append(get_close_matches(word, dictonary)[0])\n",
        "corrected_query = \" \".join(corrected_query)\n",
        "print(f\"Did you mean: {corrected_query} ?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7yM7CJnIuwg",
        "outputId": "eb27506d-9b67-4988-e551-cd509fc4fdc5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Did you mean: world cup ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# s\n"
      ],
      "metadata": {
        "id": "JzWKqTMjcgbW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lecture 3: Why is NLP hard  <br>\n",
        "CODE AND SCRIPT IN YOUTUBE VIDEO AND ALSO ALL RELVANT INFORMATION INFORMATION LIKE CHATGPT SUMMARIZATION ETC"
      ],
      "metadata": {
        "id": "8I6rIxWccho7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QBq56yxGcxRR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}